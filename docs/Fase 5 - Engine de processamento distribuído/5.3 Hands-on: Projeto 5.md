# Fase 5 - Engine de Processamento Distribuído 🚀

## **Processando Dados no Data Lake com PySpark** 🌊💻

### **Objetivo** 🎯
Neste projeto, você aplicará seus conhecimentos de **processamento distribuído com PySpark**, enquanto constrói um **Data Lake** eficiente, seguindo os princípios da [Medallion Architecture](https://medium.com/@junshan0/medallion-architecture-what-why-and-how-ce07421ef06f).

### **Descrição do Projeto** 🛠️
Você será responsável por **construir a infraestrutura de um Data Lake** na **AWS** e criar um pipeline completo de processamento de dados utilizando **PySpark**. A ideia é aplicar as transformações do PySpark nas camadas **Bronze**, **Silver** e **Gold**, seguindo os princípios da arquitetura medallion. 

Os passos principais incluem:

1. **Subir via IaC os buckets para as camadas**:
    - **Bronze**: Armazenamento bruto dos dados.
    - **Silver**: Dados refinados, prontos para análise.
    - **Gold**: Dados otimizados para consumo final.

2. **Provisionar via IaC um RDS com Postgres**:
    - Crie uma instância Postgres no RDS e faça o upload de dados que achar interessante (lembre-se, o foco é o estudo, então não precisa ser uma grande quantidade de dados).

3. **Pipeline PySpark**:
    - **Camada Bronze**: Extração de dados do RDS e armazenamento na camada Bronze.
    - **Camada Silver**: Realização de transformações nos dados.
    - **Camada Gold**: Dados refinados e prontos para análise ou uso posterior.

Esse projeto é uma excelente oportunidade para consolidar conhecimentos importantes da **Engenharia de Dados**, como o uso de **processamento distribuído** e integração com **Data Lakes**. Ao rodar suas aplicações Spark, não deixe de acompanhar os **logs** para entender como o Spark está lidando com o processo e otimizar seus fluxos de dados.

### **Boa sorte e mãos à obra!** 💪
Agora é hora de colocar seus conhecimentos em prática e construir uma solução de processamento de dados escalável e eficiente. Vamos transformar teoria em resultados reais! 🎉
