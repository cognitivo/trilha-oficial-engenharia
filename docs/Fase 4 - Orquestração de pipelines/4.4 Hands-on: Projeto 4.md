# Fase 4 - TransformaÃ§Ã£o e OrquestraÃ§Ã£o de Pipelines ğŸ¬

## Hands-on: Projeto 4 - Orquestrando TransformaÃ§Ãµes com Airflow + DBT ğŸ”„

### Objetivo ğŸ¯

Neste projeto, vocÃª orquestrarÃ¡ a transformaÃ§Ã£o dos dados jÃ¡ ingeridos pelo **Airbyte** (Projeto 3) utilizando **DBT** e **Apache Airflow**, seguindo a **Arquitetura MedalhÃ£o** (Bronze â†’ Silver â†’ Gold). O objetivo Ã© criar um pipeline completo de transformaÃ§Ã£o de dados orquestrado automaticamente. ğŸš€

### Contexto ğŸ“

No **Projeto 3**, vocÃª jÃ¡ possui dados no **S3** ingeridos via **Airbyte** e consultÃ¡veis via **Amazon Athena**. Agora, vocÃª irÃ¡:

1. **Transformar esses dados** usando **DBT** seguindo a arquitetura MedalhÃ£o
2. **Orquestrar as transformaÃ§Ãµes** usando **Apache Airflow**
3. **Automatizar a execuÃ§Ã£o** com agendamento via scheduler do Airflow

Este projeto consolidarÃ¡ os conhecimentos adquiridos nos mÃ³dulos **4.1 SQL**, **4.2 DBT** e **4.3 Apache Airflow**, integrando todas as ferramentas em uma soluÃ§Ã£o funcional. ğŸ’¡

### DescriÃ§Ã£o do Projeto ğŸ› ï¸

Como **Engenheiro(a) de Dados**, vocÃª deve:

#### 1. Configurar Infraestrutura (Terraform) ğŸ—ï¸

**Utilizando a infraestrutura jÃ¡ criada no Projeto 2**, vocÃª deve:

- Criar uma **instÃ¢ncia EC2** dedicada para o **Airflow** (ou aproveitar uma existente)
- Criar um **script de inicializaÃ§Ã£o** que:
  - Instala **Docker** e **Docker Compose** na EC2
  - Configura e inicia o **Apache Airflow** via Docker
  - Instala o **DBT** diretamente na EC2 (`dbt-core` e `dbt-athena-community`)
  - Cria o diretÃ³rio `/opt/dbt/project` para o projeto DBT
  - Configura permissÃµes e variÃ¡veis de ambiente necessÃ¡rias

**Dica**: VocÃª pode usar o mesmo padrÃ£o de script de inicializaÃ§Ã£o que foi usado para o Airbyte no Projeto 3.

#### 2. Configurar DBT com Athena ğŸ”§

ApÃ³s a infraestrutura estar rodando:

- Acessar a EC2 via SSH
- Criar um projeto DBT na pasta `/opt/dbt/project`
- Configurar o arquivo `~/.dbt/profiles.yml` com as credenciais do **Athena**:
  ```yaml
  default:
    outputs:
      dev:
        type: athena
        s3_staging_dir: "s3://seu-bucket/dbt-staging/"
        schema: "default"
        database: "seu-database"
        region_name: "us-east-1"
        aws_profile_name: "default"
    target: dev
  ```
- Testar a conexÃ£o executando `dbt debug`

#### 3. Criar Models DBT (Arquitetura MedalhÃ£o) ğŸ“Š

Criar **3 models** seguindo a **Arquitetura MedalhÃ£o**:

**a) Bronze Layer** (`models/bronze/bronze_raw_data.sql`):
- Ler dados brutos diretamente do **Athena/S3** (tabelas criadas pelo Airbyte)
- Fazer apenas referÃªncia aos sources, sem transformaÃ§Ãµes
- Objetivo: Camada de dados brutos

**b) Silver Layer** (`models/silver/silver_cleaned_data.sql`):
- Ler da camada Bronze
- Aplicar limpeza de dados (remover nulos, padronizar formatos, etc.)
- Objetivo: Camada de dados limpos e validados

**c) Gold Layer** (`models/gold/gold_analytics.sql`):
- Ler da camada Silver
- Criar agregaÃ§Ãµes e resumos prontos para analytics
- Objetivo: Camada final pronta para consumo

**Estrutura esperada:**
```
/opt/dbt/project/
â”œâ”€â”€ dbt_project.yml
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ bronze_raw_data.sql
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â””â”€â”€ silver_cleaned_data.sql
â”‚   â””â”€â”€ gold/
â”‚       â””â”€â”€ gold_analytics.sql
â”œâ”€â”€ sources.yml
â””â”€â”€ macros/
```

#### 4. Criar DAG no Airflow ğŸ¬

Criar uma **DAG** no Airflow que:

- Executa o comando `dbt run` na pasta `/opt/dbt/project`
- Utiliza o **BashOperator** para executar os comandos DBT
- Configura **scheduler** para executar diariamente (ex: Ã s 02:00 UTC)

### ValidaÃ§Ã£o ğŸ§ª

Para validar que o projeto estÃ¡ funcionando corretamente:

1. âœ… A DAG executa sem erros no Airflow
2. âœ… As tabelas sÃ£o criadas no Athena apÃ³s execuÃ§Ã£o do DBT
3. âœ… O scheduler estÃ¡ configurado e executando automaticamente
4. âœ… Os dados estÃ£o sendo transformados corretamente nas 3 camadas

### PrÃ³ximos Passos ğŸš€

ApÃ³s concluir este projeto, vocÃª terÃ¡ consolidado:
- TransformaÃ§Ã£o de dados com **SQL** e **DBT**
- OrquestraÃ§Ã£o de pipelines com **Apache Airflow**
- IntegraÃ§Ã£o de ferramentas modernas da Modern Data Stack
- AplicaÃ§Ã£o prÃ¡tica da **Arquitetura MedalhÃ£o**


### Boa sorte e mÃ£os Ã  obra! ğŸ’ª
