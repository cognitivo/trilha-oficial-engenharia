# Fase 5 - Engine de processamento distribuído

## PySpark: usando Python com Apache Spark

O PySpark é a interface do Apache Spark para Python, permitindo que combine o poder do Spark para processar grandes volumes de dados, com a familiaridade e flexibilidade da linguagem Python. Ele oferece uma abordagem simplificada para processar dados em ambientes distribuídos, lidando inclusive com os já conhecidos Dataframes. Aqui você aprenderá um pouco mais da sintaxe do PySpark, além de também ter uma visão de como ele é aplicado na prática.

Como alguns materiais de estudo, você pode consumir os listados abaixo:

- [Video: como usar python com Apache Spark: guia prático de PySpark](https://www.youtube.com/watch?v=WwrX1YVmOyA)
- [Video: Mão no código como usar o Spark com PySpark](https://www.youtube.com/watch?v=0BY8KySBHwE)
- [A practical guide to PySpark](https://medium.com/@SrGrace_/a-practical-guide-to-pyspark-a5929adf54d1)
- [PySpark Overview](https://spark.apache.org/docs/latest/api/python/index.html#:~:text=PySpark%20is%20the%20Python%20API,for%20interactively%20analyzing%20your%20data.)
- [Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/index.html)