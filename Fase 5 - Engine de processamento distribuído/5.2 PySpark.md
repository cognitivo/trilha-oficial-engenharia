# Fase 5 - Engine de processamento distribuído

## PySpark: usando Python com Apache Spark

O PySpark é a interface do Apache Spark para Python, permitindo que você aproveite o poder do Spark para processar grandes conjuntos de dados, utilizando a linguagem que já domina. Ele facilita a criação de pipelines de dados, análises avançadas e integrações com ferramentas de aprendizado de máquina, tudo em um ambiente de processamento distribuído.

Nesta etapa, você será introduzido(a) aos fundamentos do PySpark e aprenderá como utilizá-lo para criar soluções de dados escaláveis e eficientes.

[Video: como usar python com Apache Spark: guia prático de PySpark](https://www.youtube.com/watch?v=WwrX1YVmOyA)
[A practical guide to PySpark](https://medium.com/@SrGrace_/a-practical-guide-to-pyspark-a5929adf54d1)