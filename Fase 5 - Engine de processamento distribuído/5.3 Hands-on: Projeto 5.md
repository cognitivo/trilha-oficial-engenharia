# Fase 5 - Engine de Processamento Distribuído

## Processando dados no Datalake com PySpark

### Objetivo
Neste projeto, você aplicará os conceitos aprendidos sobre processamento distribuído com PySpark, aliado à construção de um Data Lake seguindo os princípios da [Medallion Architecture](https://medium.com/@junshan0/medallion-architecture-what-why-and-how-ce07421ef06f).

## Descrição do projeto
Você será responsável por construir a infraestrutura de um Data Lake na AWS e criar um pipeline completo de processamento de dados utilizando o PySpark. Portanto, você deve subir via IaC os buckets referentes as camadas bronze, silver e gold. Além disso, você também deve provisionar via IaC um RDS com instância Postgres, e populá-lo com alguns dados que julgar interessante (não muitos, esse é um projeto de estudo apenas).

O objetivo desse projeto é você usar o seu conhecimento adquirido em PySpark para levar os dados do RDS para a camada bronze, realizar transformações conforme julgar interessante para a camada silver, e posteriormente, para a camada gold. Verifique conceitualmente o que são essas camadas.

Sua aplicação PySpark deve ser desenvolvida localmente e interagir com os dados da Cloud, conforme descrito.

Este projeto é uma oportunidade de consolidar conceitos fundamentais da Engenharia de Dados, como o uso de processamento distribuído. Quando rodar suas aplicações Spark, lembre-se de acompanhar os logs para entender exatamente como o Spark está lidando com o processo.

Boa sorte!