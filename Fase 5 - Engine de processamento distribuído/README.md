# Comece por Aqui

## Bem-vindo(a) à **Fase 5 da Trilha de Engenharia de Dados da Cognitivo**!

Nesta etapa, você será introduzido(a) ao processamento distribuído com o **Apache Spark** e ao uso de **PySpark**, a interface Python do Spark. Esta é uma oportunidade de explorar como processar grandes volumes de dados de maneira eficiente, utilizando o poder do processamento distribuído.

Nesta fase, você vai:
- Entender a arquitetura do Apache Spark e por que ele é tão eficiente.
- Compreender os fundamentos do Apache Spark e sua aplicação no processamento distribuído de dados.
- Aprender como utilizar o PySpark para criar pipelines e transformar dados em um ambiente distribuído.
- Implementar um projeto prático que envolve a construção de um **Data Lake** baseado na arquitetura medallion, utilizando PySpark para processar dados na AWS.

Boa sorte e vamos lá!